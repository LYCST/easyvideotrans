import requests
import json
import concurrent.futures
import time
import tenacity
from .translator import Translator

DEFAULT_URL = "https://api.openai.com/v1/"
GHATGPT_TERMS_FILE = "../../../configs/gpt_terms.json"


class GPTTranslator(Translator):
    def __init__(self, api_key, model_name="gpt-3.5-turbo-0125", base_url=None, proxies=None, terms_file=GHATGPT_TERMS_FILE, cache_dir="./translation_cache"):
        super().__init__(cache_dir)
        self.api_key = api_key
        self.base_url = base_url if base_url else DEFAULT_URL
        self.model_name = model_name
        self.proxies = proxies
        self.terms = {}
        self.load_terms(terms_file)
    
    def get_translator_name(self):
        return f"gpt_{self.model_name.replace(':', '_').replace('-', '_')}"

    def load_terms(self, terms_file):
        try:
            with open(terms_file, 'r', encoding='utf-8') as f:
                self.terms = json.load(f)
        except FileNotFoundError:
            print(f"Warning: Terms file {terms_file} not found, using empty terms dictionary")
            self.terms = {}

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=3, max=6),
                    stop=tenacity.stop_after_attempt(3),
                    reraise=True)
    def request_llm(self, system_text="",
                    assistant_text='',
                    user_text="",
                    max_tokens=1200):
        headers = {
            "Content-Type": "application/json",
        }
        
        # åªæœ‰åœ¨ä½¿ç”¨OpenAIå®˜æ–¹APIæ—¶æ‰æ·»åŠ Authorizationå¤´
        if self.base_url == DEFAULT_URL or "openai.com" in self.base_url:
            headers["Authorization"] = f"Bearer {self.api_key}"
        elif self.api_key and self.api_key != "":  # æœ¬åœ°éƒ¨ç½²å¯èƒ½éœ€è¦APIå¯†é’¥
            headers["Authorization"] = f"Bearer {self.api_key}"

        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system",
                    "content": system_text,
                },
                {
                    "role": "assistant",
                    "content": assistant_text,
                },
                {
                    "role": "user",
                    "content": user_text,
                }
            ],
            "max_tokens": max_tokens
        }
        
        # ç¡®ä¿URLä»¥/ç»“å°¾
        api_url = self.base_url.rstrip('/') + "/chat/completions"
        
        # ç§»é™¤è°ƒè¯•æ‰“å°ï¼Œåªä¿ç•™è¿›åº¦ä¿¡æ¯
        
        response = requests.post(api_url, headers=headers, json=payload, proxies=self.proxies, timeout=60)
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code != 200:
            raise Exception(f"API request failed: {response.status_code} - {response.text}")
        
        try:
            return response.json()
        except json.JSONDecodeError as e:
            raise Exception(f"Invalid JSON response: {e}")

    def process_text(self, text, max_tokens):
        st = time.time()
        
        system_text = ("You are a professional subtitle translator that translates English subtitles to idiomatic "
                       "Chinese subtitles.")
        assistant_text = f"Here are some key terms and their translations:\n{json.dumps(self.terms, ensure_ascii=False)}"
        user_text = f"""You are a professional video subtitle translator.
        You need to translate a segment of subtitles and correct obvious word errors based on the context.
        Additionally, you need to consider some translation rules for terminology provided above.
        Below is the subtitle segment that needs to be translated:\n\n
        ```
        {text}
        ```
        your output format should be:
        ```
        translated text
        ```
        """

        results = self.request_llm(system_text=system_text,
                                   assistant_text=assistant_text,
                                   user_text=user_text,
                                   max_tokens=max_tokens)
        et = time.time()
        
        # å¤„ç†Ollama APIå“åº”æ ¼å¼
        if 'choices' in results and len(results['choices']) > 0:
            text_result = results['choices'][0]['message']['content']
        elif 'response' in results:
            # Ollama APIæ ¼å¼
            text_result = results['response']
        else:
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
            if isinstance(results, dict) and 'message' in results:
                text_result = results['message'].get('content', '')
            else:
                text_result = str(results)
        
        # å¤„ç†markdownæ ¼å¼
        if '```' in text_result:
            text_result = text_result.split('```')[1]
            text_result = text_result.strip().replace("\n", "").replace("translated text", "").replace("```", "")
        
        # å¤„ç†æœ¬åœ°éƒ¨ç½²å¯èƒ½æ²¡æœ‰usageä¿¡æ¯çš„æƒ…å†µ
        usage = results.get('usage', {})
        model = results.get('model', self.model_name)
        
        # ç§»é™¤ç»Ÿè®¡ä¿¡æ¯æ‰“å°ï¼Œåªä¿ç•™è¿›åº¦ä¿¡æ¯
        
        result_dict = {"text_result": text_result,
                       "model": model,
                       "usage": usage,
                       "all_usage": usage.get('prompt_tokens', 0) + usage.get('completion_tokens', 0) * 3 if usage else 0,
                       'time': et - st}
        return result_dict

    def translate_en_to_zh(self, texts, max_tokens=1200, max_workers=30, timeout=300):
        """
        ç¿»è¯‘è‹±æ–‡æ–‡æœ¬åˆ°ä¸­æ–‡
        
        Args:
            texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ
        """
        print(f"ğŸš€ å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {executor.submit(self.process_text, text, max_tokens): i for i, text in enumerate(texts)}
            results = [None] * len(texts)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨
            completed_count = 0
            failed_indices = []  # è®°å½•å¤±è´¥çš„ç´¢å¼•
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼Œå¸¦è¶…æ—¶
            try:
                for future in concurrent.futures.as_completed(futures, timeout=timeout):
                    try:
                        result = future.result()
                        index = futures[future]
                        results[index] = result
                        completed_count += 1
                        
                        # æ‰“å°è¿›åº¦
                        remaining = len(texts) - completed_count
                        print(f"âœ… å®Œæˆ {completed_count}/{len(texts)} ä¸ªç¿»è¯‘è¯·æ±‚ï¼Œè¿˜å‰© {remaining} ä¸ªè¯·æ±‚")
                        
                        if remaining == 0:
                            print("ğŸ‰ æ‰€æœ‰ç¿»è¯‘è¯·æ±‚å·²å®Œæˆï¼")
                        
                    except Exception as e:
                        index = futures[future]
                        print(f"âŒ ç¬¬ {index + 1} ä¸ªç¿»è¯‘è¯·æ±‚å¤±è´¥: {e}")
                        # è®°å½•å¤±è´¥çš„ç´¢å¼•ï¼Œç¨åé‡è¯•
                        failed_indices.append(index)
                        completed_count += 1
                        
            except concurrent.futures.TimeoutError:
                print(f"â° ç¿»è¯‘è¶…æ—¶ï¼å·²å®Œæˆ {completed_count}/{len(texts)} ä¸ªè¯·æ±‚")
                # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                for future in futures:
                    if not future.done():
                        future.cancel()
                        print(f"âŒ å–æ¶ˆæœªå®Œæˆçš„ç¿»è¯‘è¯·æ±‚")
                
                # è®°å½•æœªå®Œæˆçš„ä»»åŠ¡ç´¢å¼•
                for future, index in futures.items():
                    if results[index] is None:
                        failed_indices.append(index)
                        print(f"âš ï¸ ç¬¬ {index + 1} ä¸ªè¯·æ±‚å› è¶…æ—¶è€Œå¤±è´¥")
            
            # å¤„ç†å¤±è´¥çš„ç¿»è¯‘è¯·æ±‚ï¼Œå°è¯•é‡è¯•
            if failed_indices:
                print(f"ğŸ”„ å¼€å§‹é‡è¯• {len(failed_indices)} ä¸ªå¤±è´¥çš„ç¿»è¯‘è¯·æ±‚...")
                retry_results = self._retry_failed_translations(texts, failed_indices, max_tokens)
                
                # æ›´æ–°å¤±è´¥çš„ç»“æœ
                for i, failed_index in enumerate(failed_indices):
                    if retry_results[i]:
                        results[failed_index] = retry_results[i]
                        print(f"âœ… ç¬¬ {failed_index + 1} ä¸ªè¯·æ±‚é‡è¯•æˆåŠŸ")
                    else:
                        # é‡è¯•3æ¬¡éƒ½å¤±è´¥äº†ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡
                        original_text = texts[failed_index]
                        results[failed_index] = {
                            "text_result": original_text,
                            "model": self.model_name,
                            "usage": {},
                            "all_usage": 0,
                            "time": 0
                        }
                        print(f"âŒ ç¬¬ {failed_index + 1} ä¸ªè¯·æ±‚é‡è¯•3æ¬¡åä»å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡")
        
        return [result['text_result'] for result in results]

    def _retry_failed_translations(self, texts, failed_indices, max_tokens, max_retries=3):
        """
        é‡è¯•å¤±è´¥çš„ç¿»è¯‘è¯·æ±‚
        
        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            failed_indices: å¤±è´¥çš„ç´¢å¼•åˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            list: é‡è¯•ç»“æœåˆ—è¡¨
        """
        retry_results = [None] * len(failed_indices)
        
        for retry_attempt in range(max_retries):
            print(f"ğŸ”„ ç¬¬ {retry_attempt + 1} æ¬¡é‡è¯•...")
            
            for i, failed_index in enumerate(failed_indices):
                if retry_results[i] is None:  # åªé‡è¯•å°šæœªæˆåŠŸçš„
                    try:
                        # å¢åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                        import time
                        time.sleep(1)
                        
                        result = self.process_text(texts[failed_index], max_tokens)
                        retry_results[i] = result
                        print(f"âœ… é‡è¯•æˆåŠŸ: ç¬¬ {failed_index + 1} ä¸ªè¯·æ±‚")
                        
                    except Exception as e:
                        print(f"âŒ é‡è¯•å¤±è´¥: ç¬¬ {failed_index + 1} ä¸ªè¯·æ±‚ - {e}")
                        if retry_attempt == max_retries - 1:  # æœ€åä¸€æ¬¡é‡è¯•
                            retry_results[i] = None
        
        return retry_results
